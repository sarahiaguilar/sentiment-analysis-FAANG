{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zci4zQDUH91w",
        "colab_type": "text"
      },
      "source": [
        "## Load data and filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfom4YsCJPln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNg2Xphgmela",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d66dd304-1a71-49d2-a889-b9abdeed3c61"
      },
      "source": [
        "df = pd.read_csv('./data.csv')\n",
        "df.shape"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15924, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3c6co4cH77L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[df.Favorites > 3]\n",
        "df = df.drop_duplicates(subset = 'Text', ignore_index = True)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2dSwmiHIDIW",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning data and removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nJCVrB6TOz-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "346cd930-ebe7-42fa-8a07-5fd0788a957c"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "english_stopwords = set(stopwords.words(\"english\"))\n",
        "from nltk.corpus import stopwords "
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ6xEx7ATO-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "  text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text) # Remove URLs\n",
        "  text = re.sub('@[^\\s]+', '', text) # Remove usernames\n",
        "  text = re.sub(r'#([^\\s]+)', r'\\1', text) # Remove the # in #hashtag\n",
        "  text = re.sub(r'[^A-Za-z]+', ' ', text) # Remove special characters and numbers\n",
        "  text = re.sub(r'rt|fb|nflx|goog|googl|axp|aapl', '', text, flags = re.I) # Remove tickers\n",
        "  text = re.sub(r'\\b[a-zA-Z]\\b', '', text) # Remove \"single-worders\"\n",
        "  text = re.sub(r' [ ]+', ' ', text) # Remove extra whitespaces\n",
        "  text = text.lower() # Convert text to lower-case\n",
        "  return text"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5l2EOwhUOis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def removestopwords(text):\n",
        "  tokens = nltk.word_tokenize(text, 'english')\n",
        "  filtered_tokens = [i for i in tokens if i not in english_stopwords]\n",
        "  text = ' '.join(filtered_tokens)\n",
        "  return text"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xto1hw_H4lj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Text_clean'] = df['Text'].map(lambda x: clean_text(x))\n",
        "df['Text_clean'] = df['Text_clean'].map(lambda x: remove_stopwords(x))"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSv0ADu-86a1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84ac437a-d649-4f6b-e4e3-1ed9baf03f4b"
      },
      "source": [
        "df = df.dropna(subset = ['Text_clean'])\n",
        "df = df[(df.Text_clean != '')]\n",
        "df.shape"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15863, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5VjJnUP4T0q",
        "colab_type": "code",
        "colab": {},
        "cellView": "code"
      },
      "source": [
        "# from nltk.stem import PorterStemmer\n",
        "# st = PorterStemmer()\n",
        "# df['Text_clean'] = df['Text_clean'].apply(lambda x: ' '.join([st.stem(word) for word in x.split()]))"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wy9eJqCILDM",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo37MewaJWnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nuu9gKj1zaGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e6380332-3f94-4783-e43d-c7198080f12e"
      },
      "source": [
        "sid = SentimentIntensityAnalyzer()\n",
        "df['Sentiment_score'] = df.apply(lambda row: sid.polarity_scores(row['Text_clean'])['compound'], axis = 1)\n",
        "df['Sentiment'] = df.apply(lambda row: 1 if row['Sentiment_score'] > 0 else 0, axis = 1)\n",
        "sum(df.Sentiment)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8429"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swS-KWlMIUtq",
        "colab_type": "text"
      },
      "source": [
        "## Export cleaned data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G58kWBn-zaNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[['Text_clean', 'Sentiment']]\n",
        "df.rename(columns = {'Text_clean': 'Text'}, inplace = True)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeiDeY0Sucxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "outputId": "f7d91d15-711a-4a1c-9b2b-1a0eae65f288"
      },
      "source": [
        "df.head(15)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>overwhelmingly positive reviews grab friend ge...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>moved ahead hold still like cha though</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lenovo legion chance rise rest unmatched perfo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>list means jpm gs trv csco msft mcd hd cvx xom...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>expose penny stock scams learn spot twtr</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>holidays dressed hebrew santa gave away love twtr</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>investors worry big fat list risk factors djia...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>million yachts look like</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>cookies cream counteops custom cabinets brass ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>weekly coil earnings january</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>follow tips save splurge remodel discoverperso...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>faang new year extended amp losing upside mome...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>stock market trading dangerous sure avoid comm...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>holiday sales emerald diamond yearly available...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>today newyearsday means need ask current tradi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Text  Sentiment\n",
              "0   overwhelmingly positive reviews grab friend ge...          1\n",
              "1              moved ahead hold still like cha though          1\n",
              "2   lenovo legion chance rise rest unmatched perfo...          0\n",
              "3   list means jpm gs trv csco msft mcd hd cvx xom...          0\n",
              "4            expose penny stock scams learn spot twtr          0\n",
              "5   holidays dressed hebrew santa gave away love twtr          1\n",
              "6   investors worry big fat list risk factors djia...          0\n",
              "7                            million yachts look like          1\n",
              "8   cookies cream counteops custom cabinets brass ...          1\n",
              "9                        weekly coil earnings january          0\n",
              "10  follow tips save splurge remodel discoverperso...          1\n",
              "11  faang new year extended amp losing upside mome...          0\n",
              "12  stock market trading dangerous sure avoid comm...          0\n",
              "13  holiday sales emerald diamond yearly available...          1\n",
              "14  today newyearsday means need ask current tradi...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtkQF2yuq37p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('clean_data.csv', index = False)"
      ],
      "execution_count": 136,
      "outputs": []
    }
  ]
}